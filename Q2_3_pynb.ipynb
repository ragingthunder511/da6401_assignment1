{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ragingthunder511/da6401_assignment1/blob/main/Q2_3_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScI246LipR-",
        "outputId": "fe539721-008e-41c2-ded0-6af1458fb569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "LqXkOQSZRcrD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0GoFUzcDT9b",
        "outputId": "5d398383-9883-4998-da07-2f6de3850a2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "E8yCjmiKFh8I"
      },
      "outputs": [],
      "source": [
        "class Start:\n",
        "  def __init__(self):\n",
        "    training_data,testing_data = fashion_mnist.load_data()\n",
        "    self.train_x,self.train_y=training_data\n",
        "    self.test_x,self.test_y=testing_data\n",
        "\n",
        "    self.val_x = None\n",
        "    self.val_y = None\n",
        "\n",
        "\n",
        "  def data(self):\n",
        "    return self.train_x,self.train_y,self.test_x,self.test_y\n",
        "\n",
        "  def split_data(self):\n",
        "    split_index = int(0.9 * self.train_x.shape[0])\n",
        "    self.x_train_split, self.val_x =self. train_x[:split_index], self.train_x[split_index:]\n",
        "    self.y_train_split, self.val_y =self. train_y[:split_index], self.train_y[split_index:]\n",
        "    return self.x_train_split,self.val_x,self.y_train_split,self.val_y\n",
        "\n",
        "  def modified_data(self):\n",
        "    train_x=self.x_train_split.reshape(-1,28*28)/255.0\n",
        "    test_x=self.test_x.reshape(-1,28*28)/255.0\n",
        "    val_x=self.val_x.reshape(-1,28*28)/255.0\n",
        "    train_y=np.eye(10)[self.y_train_split]\n",
        "    test_y=np.eye(10)[self.test_y]\n",
        "    val_y=np.eye(10)[self.val_y]\n",
        "    return train_x,test_x,val_x,train_y,test_y,val_y\n",
        "\n",
        "  def normalize_data(self):\n",
        "    train_x=self.x_train_split.reshape(-1,28*28)/255.0\n",
        "    test_x=self.test_x.reshape(-1,28*28)/255.0\n",
        "    val_x=self.val_x.reshape(-1,28*28)/255.0\n",
        "    return train_x,test_x,val_x\n",
        "\n",
        "  # def show_data_samples(self):\n",
        "  #   samples = {class_id: None for class_id in range(10)}\n",
        "  #   count=0\n",
        "  # # Class names for Fashion-MNIST\n",
        "  #   classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  # # Iterate through the training data to find one sample for each class\n",
        "  #   for image, label in zip(self.train_x, self.train_y):\n",
        "  #     if samples[label] is None:\n",
        "  #       samples[label] = image\n",
        "  #       count=count+1;\n",
        "  #     if count==10: break\n",
        "\n",
        "  #   wandb_images = []\n",
        "  #   for i in range(10):\n",
        "  #       wandb_images.append(wandb.Image(samples[i], caption=classes[i]))\n",
        "\n",
        "  #   wandb.log({\"Fashion-MNIST Samples\": wandb_images})\n",
        "\n",
        "  # # Plot the images in a grid\n",
        "  #   plt.figure(figsize=(10, 5))\n",
        "  #   for i in range(10):\n",
        "  #     plt.subplot(2, 5, i + 1)\n",
        "  #     plt.title(classes[i],fontsize=14,fontstyle='italic')\n",
        "  #     plt.imshow(samples[i], cmap='gray')\n",
        "  #     plt.axis('off')\n",
        "  #   plt.suptitle('Fashion-MNIST Samples', fontsize=17,fontweight='bold')\n",
        "  #   plt.show()\n",
        "\n",
        "  def show_data_samples(self, sweep_name=\"sample_images_plot\"):\n",
        "    samples = {class_id: None for class_id in range(10)}\n",
        "    count = 0\n",
        "\n",
        "      # Class names for Fashion-MNIST\n",
        "    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "      # Collect one sample for each class\n",
        "    for image, label in zip(self.train_x, self.train_y):\n",
        "          if samples[label] is None:\n",
        "              samples[label] = image\n",
        "              count += 1\n",
        "          if count == 10:\n",
        "              break\n",
        "\n",
        "      # Plot all samples in a single grid image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, class_id in enumerate(samples.keys()):\n",
        "          plt.subplot(2, 5, i + 1)\n",
        "          plt.imshow(samples[class_id], cmap='gray')\n",
        "          plt.title(classes[class_id], fontsize=12, fontstyle='italic')\n",
        "          plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Fashion-MNIST Samples ({sweep_name})', fontsize=16, fontweight='bold')\n",
        "\n",
        "      # Save the image and log it in WandB\n",
        "    plt.savefig(\"fashion_mnist_samples.png\")\n",
        "    wandb.log({\"Fashion-MNIST Samples\": wandb.Image(\"fashion_mnist_samples.png\")})\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 : CLASSES REQUIRED FOR MLFFNN ( Question 2 and Question 3 )"
      ],
      "metadata": {
        "id": "2UYIeduaEeWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "nCI04CZhBmGW"
      },
      "outputs": [],
      "source": [
        "#Activation Functions\n",
        "class Activation_Functions:\n",
        "    def __init__(self, act):\n",
        "        self.af = act.lower()\n",
        "\n",
        "    #For Hidden Layers\n",
        "    def activation_function(self, z):\n",
        "        if self.af == 'relu':\n",
        "            return np.maximum(0, z)\n",
        "        elif self.af == 'tanh':\n",
        "            z=np.clip(z,-500,500)\n",
        "            return np.tanh(z)\n",
        "        elif self.af == 'sigmoid':\n",
        "            z=np.clip(z,-500,500)\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "        elif self.af == 'identity':\n",
        "            return z\n",
        "\n",
        "    def activation_derivative(self, a):\n",
        "        if self.af == 'relu':\n",
        "            return (a > 0).astype(float)\n",
        "        elif self.af == 'tanh':\n",
        "            return 1 - a ** 2\n",
        "        elif self.af == 'sigmoid':\n",
        "            return a * (1 - a)\n",
        "        elif self.af == 'identity':\n",
        "            return np.ones_like(a)\n",
        "\n",
        "    def ad2(self, z):\n",
        "        if self.af == 'relu':\n",
        "            return (z > 0).astype(int)\n",
        "        elif self.af == 'tanh':\n",
        "            ac = self.activation_function(z)\n",
        "            return 1 - ac ** 2\n",
        "        elif self.af == 'sigmoid':\n",
        "            ac = self.activation_function(z)\n",
        "            return ac * (1 - ac)\n",
        "        elif self.af == 'identity':\n",
        "            return np.ones_like(z)\n",
        "    #For Output Layer\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "#Intialization methods : To intialize weights\n",
        "class Initializer_Methods:\n",
        "  def __init__(self,m):\n",
        "    self.mthd=m.lower()\n",
        "\n",
        "  def initialize_weights(self, fin, fout, act):\n",
        "       if self.mthd == 'xavier':\n",
        "        if(act=='relu'):\n",
        "          return np.random.randn(fin, fout) * np.sqrt(2 / fin)\n",
        "        return np.random.randn(fin, fout) * np.sqrt(1 / fin)\n",
        "       elif self.mthd == 'random':\n",
        "        return np.random.randn(fin, fout) * 0.01\n",
        "\n",
        "#Error functions and Accuracy calculation\n",
        "class Eval_Metrics:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def cross_entropy_loss(self, y_true, y_pred):\n",
        "      return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
        "\n",
        "  def squared_error(self,y_true,y_pred):\n",
        "      return np.mean(np.sum(np.square(y_true - y_pred), axis=1))\n",
        "\n",
        "  def accuracy(self, y_true, y_pred):\n",
        "      return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2: IMPLEMENTATION OF FEED FORWARD"
      ],
      "metadata": {
        "id": "TEPH6vwzqdx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FF:\n",
        "  def __init__(self, input_size, hidden_layers, output_size, activation='tanh', weight_init='random' , error_type='cross'):\n",
        "    self.layers = [input_size] + hidden_layers + [output_size]\n",
        "    self.activation = activation.lower()\n",
        "    self.error=error_type\n",
        "\n",
        "    ac=Activation_Functions(self.activation)\n",
        "    self.af=ac.activation_function\n",
        "    self.softmax=ac.softmax\n",
        "\n",
        "    w_init=Initializer_Methods(weight_init)\n",
        "    self.initialize_weights = w_init.initialize_weights\n",
        "\n",
        "    eval=Eval_Metrics()\n",
        "    self.cross_entropy_loss=eval.cross_entropy_loss\n",
        "\n",
        "    self.weights = [self.initialize_weights(self.layers[i], self.layers[i + 1], self.activation) for i in range(len(self.layers) - 1)]\n",
        "    self.biases = [np.zeros((1, self.layers[i + 1])) for i in range(len(self.layers) - 1)]\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        a = x\n",
        "        self.activations = [a]\n",
        "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            a = self.af(np.dot(a, w) + b)\n",
        "            self.activations.append(a)\n",
        "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        self.activations.append(output)\n",
        "        return self.softmax(output)\n"
      ],
      "metadata": {
        "id": "3CqRTiiwqq1G"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of question 2 for 10 training data samples\n"
      ],
      "metadata": {
        "id": "dnYBVGjNwAli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "hidden_size = 3\n",
        "hidden = [128]*hidden_size\n",
        "output_size=10\n",
        "input_size = 784\n",
        "nn = FF(input_size,hidden,output_size)\n",
        "\n",
        "x=Start()\n",
        "trainx,testx,trainy,testy=x.data()\n",
        "trainx,valx,trainy,valy=x.split_data()\n",
        "x_train,x_test,x_val,y_train,y_test,y_val = x.modified_data()\n",
        "\n",
        "# Generate a dummy Fashion-MNIST image (flattened)\n",
        "x_sample = np.random.rand(1, 28*28)  # A single image\n",
        "for i in range(10):\n",
        "  output_probs = nn.forward(x_train[i][np.newaxis, :])\n",
        "  print(\"For \",i+1,\"th training sample , the output probabilities are:\", output_probs)\n",
        "  print(\"For verification: sum of probabilities = \",np.sum(output_probs))\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etBvjPUftDM1",
        "outputId": "37cb7ea9-5d08-4bb7-bd75-e617c5d3647a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For  1 th training sample , the output probabilities are: [[0.10000906 0.0999721  0.10004404 0.09998662 0.09998444 0.10002164\n",
            "  0.09998907 0.10000904 0.09997316 0.10001083]]\n",
            "For verification: sum of probabilities =  1.0000000000000002\n",
            "\n",
            "For  2 th training sample , the output probabilities are: [[0.09999711 0.1000084  0.10003889 0.09999799 0.099994   0.10002859\n",
            "  0.09996909 0.09998656 0.09996441 0.10001496]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  3 th training sample , the output probabilities are: [[0.10000802 0.10000936 0.10001058 0.10000148 0.10001257 0.10001268\n",
            "  0.09998371 0.0999868  0.09997144 0.10000335]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  4 th training sample , the output probabilities are: [[0.10000799 0.10000812 0.1000094  0.09999928 0.1000048  0.10002651\n",
            "  0.09997719 0.09998164 0.09997414 0.10001094]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  5 th training sample , the output probabilities are: [[0.10000743 0.10002159 0.10003976 0.10000289 0.10000841 0.10002444\n",
            "  0.09995838 0.09996894 0.09995907 0.1000091 ]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  6 th training sample , the output probabilities are: [[0.10000466 0.10001017 0.10002999 0.09997952 0.10001046 0.10003042\n",
            "  0.099966   0.09999506 0.09997826 0.09999545]]\n",
            "For verification: sum of probabilities =  0.9999999999999999\n",
            "\n",
            "For  7 th training sample , the output probabilities are: [[0.10002109 0.09998257 0.10002465 0.10000606 0.10000254 0.09999365\n",
            "  0.09999084 0.09999795 0.09997899 0.10000165]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  8 th training sample , the output probabilities are: [[0.10002158 0.09998944 0.10003529 0.09998421 0.10000604 0.10000121\n",
            "  0.0999785  0.10001382 0.0999817  0.09998822]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  9 th training sample , the output probabilities are: [[0.09999298 0.10000459 0.10001771 0.10000169 0.09999058 0.10002511\n",
            "  0.09998761 0.09998073 0.09998697 0.10001203]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n",
            "For  10 th training sample , the output probabilities are: [[0.09998117 0.10000174 0.10001435 0.09999153 0.09999828 0.10001825\n",
            "  0.09999127 0.09997899 0.0999987  0.10002573]]\n",
            "For verification: sum of probabilities =  1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 3: IMPLEMENTATION OF FFNN MODEL"
      ],
      "metadata": {
        "id": "TWqvDwHKEbWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hP_UVPSQFLPu"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN:\n",
        "    #Initialization of class data\n",
        "    def __init__(self, input_size, hidden_layers, output_size, learning_rate=0.001, activation='sigmoid', weight_init='random', weight_decay=0.0 , error_type='cross'):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with given layer sizes and configurations.\n",
        "        :param input_size: Number of input neurons (e.g., 28x28 for Fashion-MNIST)\n",
        "        :param hidden_layers: List containing the number of neurons in each hidden layer\n",
        "        :param output_size: Number of output neurons (10 for Fashion-MNIST classes)\n",
        "        :param learning_rate: Learning rate for gradient descent\n",
        "        :param activation: Activation function for hidden layers ('relu', 'tanh', 'sigmoid')\n",
        "        :param weight_init: Weight initialization method ('random', 'xavier')\n",
        "        :param weight_decay: L2 regularization parameter\n",
        "        \"\"\"\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.activation = activation.lower()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.error=error_type\n",
        "        self.weight_init=weight_init\n",
        "        self.t = 0\n",
        "        self.activations = []\n",
        "        self.d_weights = []\n",
        "        self.d_biases = []\n",
        "\n",
        "\n",
        "        ac=Activation_Functions(self.activation)\n",
        "        self.af=ac.activation_function\n",
        "        self.ad1=ac.activation_derivative\n",
        "        self.ad2=ac.ad2\n",
        "        self.softmax=ac.softmax\n",
        "\n",
        "        w_init=Initializer_Methods(self.weight_init)\n",
        "        self.initialize_weights = w_init.initialize_weights\n",
        "\n",
        "        eval=Eval_Metrics()\n",
        "        self.cross_entropy_loss=eval.cross_entropy_loss\n",
        "        self.squared_error=eval.squared_error\n",
        "        self.accuracy=eval.accuracy\n",
        "\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = [self.initialize_weights(self.layers[i], self.layers[i + 1], weight_init) for i in range(len(self.layers) - 1)]\n",
        "        self.biases = [np.zeros((1, self.layers[i + 1])) for i in range(len(self.layers) - 1)]\n",
        "\n",
        "        # For the optimizers, initialize the necessary variables\n",
        "        self.momentums = [np.zeros_like(w) for w in self.weights]\n",
        "        self.biases_momentums = [np.zeros_like(b) for b in self.biases]\n",
        "        self.velocity = [np.zeros_like(w) for w in self.weights]\n",
        "        self.squared_gradients = [np.zeros_like(w) for w in self.weights]\n",
        "        self.squared_biases = [np.zeros_like(w) for w in self.biases]\n",
        "        self.m = [np.zeros_like(w) for w in self.weights]\n",
        "        self.v = [np.zeros_like(w) for w in self.weights]\n",
        "        self.biases_m = [np.zeros_like(w) for w in self.biases]\n",
        "        self.biases_v = [np.zeros_like(w) for w in self.biases]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        a = x\n",
        "        self.activations = [a]\n",
        "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            a = self.af(np.dot(a, w) + b)\n",
        "            self.activations.append(a)\n",
        "        output = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        self.activations.append(output)\n",
        "        return self.softmax(output)\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        m = x.shape[0]\n",
        "        if(self.error=='cross'):\n",
        "          d_output = self.activations[-1] - y\n",
        "        elif(self.error=='squared'):\n",
        "          d_output =( (self.activations[-1] - y)*2 ) / m\n",
        "\n",
        "        d_w = np.dot(self.activations[-2].T, d_output) / m + self.weight_decay * self.weights[-1]\n",
        "        d_b = np.sum(d_output, axis=0, keepdims=True) / m\n",
        "\n",
        "        self.d_weights = [d_w]\n",
        "        self.d_biases = [d_b]\n",
        "\n",
        "        for i in range(len(self.layers) - 3, -1, -1):\n",
        "            #d_activation = np.dot(d_output, self.weights[i + 1].T) * self.activation_derivative(np.dot(self.activations[i], self.weights[i]) + self.biases[i])\n",
        "            d_activation = np.dot(d_output, self.weights[i + 1].T) * self.ad2(self.activations[i + 1])\n",
        "            d_output = d_activation\n",
        "            d_w = np.dot(self.activations[i].T, d_output) / m + self.weight_decay * self.weights[i]\n",
        "            d_b = np.sum(d_output, axis=0, keepdims=True) / m\n",
        "            self.d_weights.insert(0, d_w)\n",
        "            self.d_biases.insert(0, d_b)\n",
        "\n",
        "    def update_weights(self, optimizer=\"sgd\"):\n",
        "        \"\"\"Update weights and biases using the specified optimization algorithm.\"\"\"\n",
        "        if optimizer == \"sgd\":\n",
        "            self.sgd_optimizer()\n",
        "        elif optimizer == \"momentum\":\n",
        "            self.momentum_optimizer()\n",
        "        elif optimizer == \"nesterov\":\n",
        "            self.nesterov_optimizer()\n",
        "        elif optimizer == \"rmsprop\":\n",
        "            self.rmsprop_optimizer()\n",
        "        elif optimizer == \"adam\":\n",
        "            self.adam_optimizer()\n",
        "        elif optimizer == \"nadam\":\n",
        "            self.nadam_optimizer()\n",
        "\n",
        "    #Optimizers\n",
        "    def sgd_optimizer(self):\n",
        "        \"\"\"SGD update rule.\"\"\"\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * (self.d_weights[i]+self.weight_decay*self.weights[i])\n",
        "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "   # def momentum_optimizer(self, beta=0.9):\n",
        "   #    \"\"\"Momentum-based gradient descent update.\"\"\"\n",
        "   #    for i in range(len(self.weights)):\n",
        "   #        self.momentums[i] = beta * self.momentums[i] + (1 - beta) * self.d_weights[i]\n",
        "   #        self.weights[i] -= self.learning_rate * self.momentums[i]\n",
        "   #        self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "\n",
        "    #def momentum_optimizer(self, beta=0.9):\n",
        "    #  \"\"\"Momentum-based gradient descent update.\"\"\"\n",
        "    #  for i in range(len(self.weights)):\n",
        "    #      self.momentums[i] = beta * self.momentums[i] - self.learning_rate * self.d_weights[i]\n",
        "    #      self.weights[i] += self.momentums[i]\n",
        "    #      self.biases_momentums[i] = beta * self.biases_momentums[i] - self.learning_rate * self.d_biases[i]\n",
        "    #      self.biases[i] += self.biases_momentums[i]\n",
        "\n",
        "    def momentum_optimizer(self, beta=0.9):\n",
        "      \"\"\"Momentum-based gradient descent update.\"\"\"\n",
        "      for i in range(len(self.weights)):\n",
        "          self.momentums[i] = beta * self.momentums[i] + self.learning_rate * (self.d_weights[i]+self.weight_decay*self.weights[i])\n",
        "          self.weights[i] -= self.learning_rate * self.momentums[i]\n",
        "          self.biases_momentums[i] = beta * self.biases_momentums[i] + self.learning_rate * self.d_biases[i]\n",
        "          self.biases[i] -= self.learning_rate * self.biases_momentums[i]\n",
        "\n",
        "\n",
        "    def nesterov_optimizer(self, beta=0.9):\n",
        "      \"\"\"Nesterov Accelerated Gradient Descent (NAG).\"\"\"\n",
        "      for i in range(len(self.weights)):\n",
        "          lookahead_weights = self.weights[i] - beta * self.momentums[i]\n",
        "          grad_w = self.d_weights[i]\n",
        "          self.momentums[i] = beta * self.momentums[i] + self.learning_rate * (grad_w + self.weight_decay*lookahead_weights)\n",
        "          self.weights[i] = lookahead_weights - self.learning_rate * self.momentums[i]\n",
        "\n",
        "          lookahead_biases = self.biases[i] - beta * self.biases_momentums[i]\n",
        "          grad_b = self.d_biases[i]\n",
        "          self.biases_momentums[i] = beta * self.biases_momentums[i] + self.learning_rate * grad_b\n",
        "          self.biases[i] = lookahead_biases - self.learning_rate * self.biases_momentums[i]\n",
        "\n",
        "        # self.momentums[i] = ( beta * self.momentums[i] ) - ( self.learning_rate * self.d_weights[i] )\n",
        "        # self.weights[i] += ( beta * self.momentums[i] ) - ( self.learning_rate * self.momentums[i] )\n",
        "        # self.biases_momentums[i] = ( beta * self.biases_momentums[i] ) - ( self.learning_rate * self.d_biases[i] )\n",
        "        # self.biases[i] += ( beta * self.biases_momentums[i] ) - ( self.learning_rate * self.biases_momentums[i] )\n",
        "\n",
        "\n",
        "    def rmsprop_optimizer(self, beta=0.9, epsilon=1e-6):\n",
        "      \"\"\"RMSprop optimizer (fixed bias update issue).\"\"\"\n",
        "      for i in range(len(self.weights)):\n",
        "          self.squared_gradients[i] = beta * self.squared_gradients[i] + (1 - beta) * (self.d_weights[i] ** 2)\n",
        "          self.squared_biases[i] = beta * self.squared_biases[i] + (1 - beta) * (self.d_biases[i] ** 2)\n",
        "          self.weights[i] -= self.learning_rate * ((self.d_weights[i]+ (self.weight_decay * self.weights[i])) / (np.sqrt(self.squared_gradients[i]) + epsilon))\n",
        "          self.biases[i] -= self.learning_rate * (self.d_biases[i] / (np.sqrt(self.squared_biases[i]) + epsilon))\n",
        "\n",
        "\n",
        "    #def adam_optimizer(self, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    #    \"\"\"Adam optimizer.\"\"\"\n",
        "    #    self.t += 1\n",
        "    #    for i in range(len(self.weights)):\n",
        "    #        self.m[i] = beta1 * self.m[i] + (1 - beta1) * self.d_weights[i]\n",
        "    #        self.v[i] = beta2 * self.v[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
        "    #        m_hat = self.m[i] / (1 - (beta1 ** self.t))\n",
        "    #        v_hat = self.v[i] / (1 - (beta2 ** self.t))\n",
        "    #        self.weights[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "    #        self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    def adam_optimizer(self, beta1=0.9, beta2=0.999, epsilon=1e-6):\n",
        "      \"\"\"Adam optimizer.\"\"\"\n",
        "      self.t += 1\n",
        "      for i in range(len(self.weights)):\n",
        "          self.m[i] = beta1 * self.m[i] + (1 - beta1) * self.d_weights[i]\n",
        "          self.v[i] = beta2 * self.v[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
        "          m_hat = self.m[i] / (1 - beta1 ** self.t)\n",
        "          v_hat = self.v[i] / (1 - beta2 ** self.t)\n",
        "          self.weights[i] -= self.learning_rate * (( m_hat /( (np.sqrt(v_hat) + epsilon)) + self.weight_decay * self.weights[i]) )\n",
        "\n",
        "          self.biases_m[i] = beta1 * self.biases_m[i] + (1 - beta1) * self.d_biases[i]\n",
        "          self.biases_v[i] = beta2 * self.biases_v[i] + (1 - beta2) * (self.d_biases[i] ** 2)\n",
        "          biases_m_hat = self.biases_m[i] / (1 - beta1 ** self.t)\n",
        "          biases_v_hat = self.biases_v[i] / (1 - beta2 ** self.t)\n",
        "          self.biases[i] -= self.learning_rate * (( biases_m_hat / (np.sqrt(biases_v_hat) + epsilon))  )\n",
        "\n",
        "    #def nadam_optimizer(self, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    #    \"\"\"Nadam optimizer (combines Nesterov and Adam).\"\"\"\n",
        "    #    self.t += 1\n",
        "    #    for i in range(len(self.weights)):\n",
        "    #        prev_momentum = self.momentums[i]\n",
        "    #        self.momentums[i] = beta1 * self.momentums[i] + (1 - beta1) * self.d_weights[i]\n",
        "    #        self.v[i] = beta2 * self.v[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
        "    #        m_hat = self.momentums[i] / (1 - (beta1 ** self.t))\n",
        "    #        v_hat = self.v[i] / (1 - (beta2 ** self.t))\n",
        "    #        self.weights[i] -= self.learning_rate * (m_hat + (1 - beta1) * prev_momentum) / (np.sqrt(v_hat) + epsilon)\n",
        "    #        self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    #def nadam_optimizer(self, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    #  \"\"\"Nadam optimizer (combining Nesterov and Adam).\"\"\"\n",
        "    #  self.t += 1\n",
        "    #  for i in range(len(self.weights)):\n",
        "    #      self.m[i] = beta1 * self.m[i] + (1 - beta1) * self.d_weights[i]\n",
        "    #      self.v[i] = beta2 * self.v[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
        "    #      m_hat = self.m[i] / (1 - (beta1 ** self.t))\n",
        "    #      v_hat = self.v[i] / (1 - (beta2 ** self.t))\n",
        "    #      nesterov_m = beta1 * m_hat + (1 - beta1) * self.d_weights[i]\n",
        "    #      self.weights[i] -= self.learning_rate * nesterov_m / (np.sqrt(v_hat) + epsilon)\n",
        "    #      self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    def nadam_optimizer(self, beta1=0.9, beta2=0.999, epsilon=1e-6):\n",
        "      \"\"\"Nadam optimizer (combining Nesterov and Adam).\"\"\"\n",
        "      self.t += 1  # Time step for bias correction\n",
        "\n",
        "      for i in range(len(self.weights)):\n",
        "          self.m[i] = beta1 * self.m[i] + (1 - beta1) * self.d_weights[i]\n",
        "          self.v[i] = beta2 * self.v[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
        "          m_hat = self.m[i] / (1 - (beta1 ** self.t))\n",
        "          v_hat = self.v[i] / (1 - (beta2 ** self.t))\n",
        "          nesterov_m = beta1 * m_hat + (1 - beta1) * self.d_weights[i]\n",
        "          self.weights[i] -= self.learning_rate * (( nesterov_m / (np.sqrt(v_hat) + epsilon))+ self.weight_decay * self.weights[i] )\n",
        "\n",
        "          self.biases_m[i] = beta1 * self.biases_m[i] + (1 - beta1) * self.d_biases[i]\n",
        "          self.biases_v[i] = beta2 * self.biases_v[i] + (1 - beta2) * (self.d_biases[i] ** 2)\n",
        "          biases_m_hat = self.biases_m[i] / (1 - (beta1 ** self.t))\n",
        "          biases_v_hat = self.biases_v[i] / (1 - (beta2 ** self.t))\n",
        "          nesterov_bias_m = beta1 * biases_m_hat + (1 - beta1) * self.d_biases[i]\n",
        "          self.biases[i] -= self.learning_rate *( nesterov_bias_m / (np.sqrt(biases_v_hat) + epsilon) )\n",
        "\n",
        "\n",
        "    # def train(self, x_train, y_train, epochs, batch_size, optimizer=\"sgd\", validation_data=None):\n",
        "    #     num_samples = x_train.shape[0]\n",
        "    #     for epoch in range(epochs):\n",
        "    #         total_loss = 0\n",
        "    #         total_acc = 0\n",
        "    #         permutation = np.random.permutation(num_samples)\n",
        "    #         x_train = x_train[permutation]\n",
        "    #         y_train = y_train[permutation]\n",
        "\n",
        "    #         for i in range(0, num_samples, batch_size):\n",
        "    #             x_batch = x_train[i:i+batch_size]\n",
        "    #             y_batch = y_train[i:i+batch_size]\n",
        "    #             output = self.forward(x_batch)\n",
        "    #             if(self.error=='cross'):\n",
        "    #                 loss = self.cross_entropy_loss(y_batch, output)\n",
        "    #             elif(self.error=='squared'):\n",
        "    #                 loss = self.squared_error(y_batch, output)\n",
        "    #             acc = self.accuracy(y_batch, output)\n",
        "    #             total_loss += loss\n",
        "    #             total_acc += acc\n",
        "    #             self.backward(x_batch, y_batch)\n",
        "    #             self.update_weights(optimizer)\n",
        "\n",
        "    #         val_loss = None\n",
        "    #         val_acc = None\n",
        "    #         if validation_data:\n",
        "    #             x_val, y_val = validation_data\n",
        "    #             val_output = self.forward(x_val)\n",
        "    #             if(self.error=='cross'):\n",
        "    #                 val_loss = self.cross_entropy_loss(y_val, val_output)\n",
        "    #             elif(self.error=='squared'):\n",
        "    #                 val_loss = self.squared_error(y_val, val_output)\n",
        "    #             val_acc = self.accuracy(y_val, val_output)\n",
        "\n",
        "    #         epoch_no = f\"{epoch+1}/{epochs}\"\n",
        "    #         loss_no = total_loss / (num_samples // batch_size)\n",
        "    #         acc_no = total_acc / (num_samples // batch_size)\n",
        "    #         if validation_data:\n",
        "    #             val_loss_no = val_loss\n",
        "    #             val_acc_no = val_acc\n",
        "    #         else:\n",
        "    #             val_loss_no = None\n",
        "    #             val_acc_no = None\n",
        "    #         #print(\"Epoch:\",epoch_no,\"Loss:\",loss_no,\"Accuracy:\",acc_no,\"Val loss:\",val_loss,\"Val_accuracy:\",val_acc)\n",
        "    #         print(f\"Epoch: {epoch+1}/{epochs}, Loss: {total_loss / (num_samples // batch_size)}, Accuracy: {(total_acc / (num_samples // batch_size))*100}, Val Loss: {val_loss}, Val Accuracy: {100*val_acc}\")\n",
        "    #         #wandb.log({\"epoch\": {epoch+1}/{epochs}, \"loss\": {total_loss / (num_samples // batch_size)}, \"accuracy\": {total_acc / (num_samples // batch_size)}, \"val_loss\": {val_loss}, \"val_acc\" : {val_acc}\"})\n",
        "    #         #wandb.log({\"epoch\": {epoch+1}/{epochs}, \"loss\": {total_loss / (num_samples // batch_size)}, \"accuracy\": {total_acc / (num_samples // batch_size)}})\n",
        "    #         #wandb.log({\"val_loss\": {val_loss}, \"val_acc\" : {val_acc}})\n",
        "    #         wandb.log({\"epoch\": epoch+1, \"loss\": loss_no, \"accuracy\": acc_no*100})\n",
        "    #         wandb.log({\"val_loss\": val_loss_no, \"val_acc\" : val_acc*100})\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, epochs, batch_size, optimizer=\"sgd\", validation_data=None):\n",
        "      num_samples = x_train.shape[0]\n",
        "\n",
        "      # Moving average for validation loss/accuracy\n",
        "      val_loss_history = []\n",
        "      val_acc_history = []\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          total_loss = 0\n",
        "          total_acc = 0\n",
        "          permutation = np.random.permutation(num_samples)\n",
        "          x_train = x_train[permutation]\n",
        "          y_train = y_train[permutation]\n",
        "\n",
        "          # Mini-batch training loop\n",
        "          for i in range(0, num_samples, batch_size):\n",
        "              x_batch = x_train[i:i+batch_size]\n",
        "              y_batch = y_train[i:i+batch_size]\n",
        "              output = self.forward(x_batch)\n",
        "\n",
        "              # Compute loss\n",
        "              if self.error == 'cross':\n",
        "                  loss = self.cross_entropy_loss(y_batch, output)\n",
        "              elif self.error == 'squared':\n",
        "                  loss = self.squared_error(y_batch, output)\n",
        "\n",
        "              acc = self.accuracy(y_batch, output)\n",
        "              total_loss += loss\n",
        "              total_acc += acc\n",
        "\n",
        "              # Backpropagation and weight update\n",
        "              self.backward(x_batch, y_batch)\n",
        "              self.update_weights(optimizer)\n",
        "\n",
        "          # Compute epoch-level training loss & accuracy\n",
        "          avg_train_loss = total_loss / (num_samples // batch_size)\n",
        "          avg_train_acc = (total_acc / (num_samples // batch_size)) * 100\n",
        "\n",
        "          # Validation computation\n",
        "          val_loss_no, val_acc_no = None, None\n",
        "          if validation_data:\n",
        "              x_val, y_val = validation_data\n",
        "              val_output = self.forward(x_val)\n",
        "\n",
        "              # Compute validation loss\n",
        "              if self.error == 'cross':\n",
        "                  val_loss = self.cross_entropy_loss(y_val, val_output)\n",
        "              elif self.error == 'squared':\n",
        "                  val_loss = self.squared_error(y_val, val_output)\n",
        "\n",
        "              val_acc = self.accuracy(y_val, val_output) * 100\n",
        "\n",
        "              # Moving average for stability\n",
        "              val_loss_history.append(val_loss)\n",
        "              val_acc_history.append(val_acc)\n",
        "\n",
        "              if len(val_loss_history) > 5:  # Maintain a window of last 5 epochs\n",
        "                  val_loss_history.pop(0)\n",
        "                  val_acc_history.pop(0)\n",
        "\n",
        "              val_loss_no = sum(val_loss_history) / len(val_loss_history)\n",
        "              val_acc_no = sum(val_acc_history) / len(val_acc_history)\n",
        "\n",
        "          # Print results\n",
        "          print(f\"Epoch: {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}, Accuracy: {avg_train_acc:.2f}%, \"\n",
        "                f\"Val Loss: {val_loss_no:.4f}, Val Accuracy: {val_acc_no:.2f}%\")\n",
        "\n",
        "          # Logging to WandB\n",
        "          wandb.log({\"epoch\": epoch+1, \"loss\": avg_train_loss, \"accuracy\": avg_train_acc})\n",
        "          if validation_data:\n",
        "              wandb.log({\"val_loss\": val_loss_no, \"val_acc\": val_acc_no})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 ends above\n"
      ],
      "metadata": {
        "id": "rdWopZ6ip67M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "otiSHNzQEV1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "-06aGhixGJm9"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(model, x_test, y_test, class_names):\n",
        "    \"\"\"Generate and log the confusion matrix in WandB.\"\"\"\n",
        "\n",
        "    # Get model predictions\n",
        "    y_pred_probs = model.forward(x_test)  # Forward pass\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)  # Convert to class labels\n",
        "    y_true = np.argmax(y_test, axis=1)  # True class labels\n",
        "\n",
        "    # Compute Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Fashion-MNIST Confusion Matrix ðŸŽ¨\")\n",
        "\n",
        "    # Saving the plot\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "    #Logging confusion matrix in WandB\n",
        "    wandb.log({\"Confusion Matrix\": wandb.Image(\"confusion_matrix.png\")})\n",
        "\n",
        "    # cm_table = wandb.Table(columns=[\"Actual\", \"Predicted\"])\n",
        "    # for actual, predicted in zip(y_true, y_pred):\n",
        "    #     cm_table.add_data(class_names[actual], class_names[predicted])\n",
        "\n",
        "    # wandb.log({\"Confusion Matrix (Table)\": cm_table})\n",
        "\n",
        "    # Close the plot to prevent memory issues\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SWEEP CODE\n"
      ],
      "metadata": {
        "id": "MUSVBws8ESF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMOoloc9J6GA",
        "outputId": "75f87acc-46fe-44f2-97c1-8bb7f578a4c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 8d1xh9o7\n",
            "Sweep URL: https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/8d1xh9o7\n"
          ]
        }
      ],
      "source": [
        "# WandB Sweep Configuration\n",
        "sweep_configuration = {\n",
        "    'method': \"bayes\",\n",
        "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'num_hidden_layers': {'values': [3, 4, 5]},\n",
        "        'hidden_layer_size': {'values': [32, 64, 128]},\n",
        "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        #'optimizer_name': {'values': ['sgd','momentum','nesterov','rmsprop','adam','nadam']},\n",
        "        'optimizer_name': {'values': ['sgd','momentum','nesterov','rmsprop','adam']},\n",
        "        #'optimizer_name': {'values': ['momentum']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'init_type': {'values': ['random', 'xavier']},\n",
        "        #'activation_type': {'values': ['sigmoid', 'tanh', 'relu', 'identity']},\n",
        "        'activation_type': {'values': ['sigmoid', 'tanh', 'relu']},\n",
        "        #'activation_type': {'values': ['tanh']},\n",
        "        #'loss_type': {'values': ['cross', 'squared']}\n",
        "        'loss_type': {'values': ['squared']}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_configuration, project=\"cs24m020_dla1_Q4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SWEEP CODE FOR BEST PERFORMING MODEL ( FOR CONFUSION MATRIX CALCULATION )"
      ],
      "metadata": {
        "id": "UWRMST00A1Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB Best Sweep Configuration\n",
        "sweep_best_configuration = {\n",
        "    'method': \"bayes\",\n",
        "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [10]},\n",
        "        'num_hidden_layers': {'values': [3]},\n",
        "        'hidden_layer_size': {'values': [128]},\n",
        "        'learning_rate': {'values': [1e-4]},\n",
        "        'weight_decay': {'values': [0]},\n",
        "        'optimizer_name': {'values': ['rmsprop']},\n",
        "        'batch_size': {'values': [32]},\n",
        "        'init_type': {'values': [ 'xavier']},\n",
        "        'activation_type': {'values': ['relu']},\n",
        "        'loss_type': {'values': ['cross']}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_best_configuration, project=\"cs24m020_dla1_Q4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URzZ1NUZC8-V",
        "outputId": "b48f891b-1ff3-47c9-a24d-90716ce2a852"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 13hw1axf\n",
            "Sweep URL: https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/13hw1axf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING SWEEPS"
      ],
      "metadata": {
        "id": "lZHL26qMA9YB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h1y34ZYJr07b",
        "outputId": "c9631e14-d388-4eeb-8207-58e95906cada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y219typp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_type: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_type: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_name: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'cs24m020_dla1' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_163803-y219typp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/runs/y219typp' target=\"_blank\">laced-sweep-1</a></strong> to <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/13hw1axf' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/13hw1axf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/13hw1axf' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/sweeps/13hw1axf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/runs/y219typp' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/runs/y219typp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Loss: 1.7884, Accuracy: 85.60%, Val Loss: 1.6563, Val Accuracy: 93.02%\n",
            "Epoch: 2/10, Loss: 1.6385, Accuracy: 92.31%, Val Loss: 1.6189, Val Accuracy: 93.98%\n",
            "Epoch: 3/10, Loss: 1.5965, Accuracy: 93.93%, Val Loss: 1.6008, Val Accuracy: 94.58%\n",
            "Epoch: 4/10, Loss: 1.5759, Accuracy: 94.80%, Val Loss: 1.5884, Val Accuracy: 95.04%\n",
            "Epoch: 5/10, Loss: 1.5624, Accuracy: 95.37%, Val Loss: 1.5781, Val Accuracy: 95.36%\n",
            "Epoch: 6/10, Loss: 1.5524, Accuracy: 95.86%, Val Loss: 1.5549, Val Accuracy: 96.15%\n",
            "Epoch: 7/10, Loss: 1.5447, Accuracy: 96.22%, Val Loss: 1.5453, Val Accuracy: 96.59%\n",
            "Epoch: 8/10, Loss: 1.5391, Accuracy: 96.44%, Val Loss: 1.5375, Val Accuracy: 96.88%\n",
            "Epoch: 9/10, Loss: 1.5339, Accuracy: 96.67%, Val Loss: 1.5322, Val Accuracy: 97.04%\n",
            "Epoch: 10/10, Loss: 1.5299, Accuracy: 96.88%, Val Loss: 1.5285, Val Accuracy: 97.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-c4d35a2b2d33>:20: UserWarning: Glyph 127912 (\\N{ARTIST PALETTE}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\"confusion_matrix.png\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>â–â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–ƒâ–„â–„â–…â–†â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–†â–…â–„â–„â–‚â–‚â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>96.875</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>1.5299</td></tr><tr><td>val_acc</td><td>97.18</td></tr><tr><td>val_loss</td><td>1.52847</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">h_128_opt_adam_ac_tanh_loss_cross_init_xavier_lr_0.0001</strong> at: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/runs/y219typp' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4/runs/y219typp</a><br> View project at: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dla1_Q4</a><br>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_163803-y219typp/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train_sweep():\n",
        "    # Initialize a new WandB run\n",
        "    wandb.init(project=\"cs24m020_dla1\",config=sweep_configuration)\n",
        "\n",
        "    # Construct a meaningful run name\n",
        "    run_name = f\"h_{wandb.config.hidden_layer_size}_opt_{wandb.config.optimizer_name}_ac_{wandb.config.activation_type}_loss_{wandb.config.loss_type}_init_{wandb.config.init_type}_lr_{wandb.config.learning_rate}\"\n",
        "\n",
        "    # Set custom run name\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()  # Save the run name\n",
        "\n",
        "    # Extract hyperparameters from the sweep config\n",
        "    config = wandb.config\n",
        "    num_hidden_layers = config.num_hidden_layers\n",
        "    hidden_layer_size = config.hidden_layer_size\n",
        "    hidden_layers = [hidden_layer_size] * num_hidden_layers  # Create hidden layer list\n",
        "    learning_rate = config.learning_rate\n",
        "    batch_size = config.batch_size\n",
        "    optimizer_name = config.optimizer_name\n",
        "    init_type = config.init_type\n",
        "    activation_type = config.activation_type\n",
        "    loss_type = config.loss_type\n",
        "    epochs = config.epochs\n",
        "    decay = config.weight_decay\n",
        "\n",
        "    x=Start()\n",
        "    trainx,testx,trainy,testy=x.data()\n",
        "    trainx,valx,trainy,valy=x.split_data()\n",
        "    x_train,x_test,x_val,y_train,y_test,y_val = x.modified_data()\n",
        "\n",
        "    # Initialize the model\n",
        "    model = FeedForwardNN(\n",
        "        input_size=28*28, hidden_layers=hidden_layers, output_size=10, learning_rate=learning_rate, activation=activation_type, weight_init=init_type, weight_decay=decay,error_type=loss_type\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.train(x_train, y_train, epochs, batch_size, optimizer=optimizer_name, validation_data=(x_val,y_val))\n",
        "\n",
        "    classes = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "    x.show_data_samples()\n",
        "    log_confusion_matrix(model, x_test, y_test, classes)\n",
        "\n",
        "# Run the sweep\n",
        "wandb.agent(sweep_id, train_sweep, count=1)\n",
        "\n",
        "#self, input_size, hidden_layers, output_size, learning_rate=0.01, activation='relu', weight_init='random', weight_decay=0.0 , error_type='cross'\n",
        "#self, x_train, y_train, epochs, batch_size, optimizer=\"sgd\", validation_data=None"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9sMc2S7BFXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBydgFSROxGgNrQcve4vq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}